{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "The goals of this course are to:\n",
    "\n",
    "-   Understand the importance of feature engineering\n",
    "-   Learn how to create new features from existing data\n",
    "-   Learn how to use domain knowledge to create new features\n",
    "-   Learn how to encode categorical variables for machine learning\n",
    "-   Learn how to handle missing values in a dataset\n",
    "-   Learn how to deal with time series data\n",
    "-   Learn how to scale features for machine learning\n",
    "-   Learn how to evaluate features\n",
    "-   Learn how to use Pandas and Scikit-Learn to engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "*Imputation* means to fill in missing values with plausible values. There are many\n",
    "options:\n",
    "\n",
    "-  A constant value that has meaning within the domain, such as 0, distinct from all\n",
    "    other values.\n",
    "-  A value from another randomly selected record.\n",
    "-  A mean, median or mode value for the column.\n",
    "-  A value estimated by another predictive model.\n",
    "\n",
    "We use imputation because many machine learning algorithms do not support missing values. Modern algorithms like XGBoost handle missing values themselves, but it is still a common practice to impute because other algorithms do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Fuel economy data from the U.S. Environmental Protection Agency (EPA) for 2019 model year vehicles. The data are available in a CSV file with 82,000 rows and 83 columns.\n",
    "\n",
    "https://www.fueleconomy.gov/feg/epadata/vehicles.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://www.fueleconomy.gov/feg/epadata/vehicles.csv' \n",
    "\n",
    "raw = pd.read_csv(url)#, dtype_backend='pyarrow', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year', 'make', 'model', 'trany', 'drive', 'VClass', 'eng_dscr',\n",
    "    'barrels08', 'city08', 'comb08', 'range', 'evMotor', 'cylinders', 'displ', 'fuelCost08', \n",
    "        'fuelType', 'highway08',  'trans_dscr','createdOn']\n",
    "\n",
    "def to_tz(df_, time_col, tz_offset, tz_name):\n",
    "    return (df_\n",
    "            .groupby(tz_offset)\n",
    "            [time_col]\n",
    "            .transform(lambda s: pd.to_datetime(s)\n",
    "                       .dt.tz_localize(s.name, ambiguous=True)\n",
    "                       .dt.tz_convert(tz_name))\n",
    "            )\n",
    "\n",
    "autos = (raw.loc[:, cols]\n",
    "         .assign(\n",
    "            offset=(raw.createdOn.str.extract(r'\\d\\d:\\d\\d (?P<offset>[A-Z]{3}?)')\n",
    "                .replace('EDT', 'EST5EDT')),\n",
    "            str_date=(raw.createdOn.str.slice(4,19) + ' ' +\n",
    "                raw.createdOn.str.slice(-4)),\n",
    "            createdOn=lambda df_: to_tz(df_, 'str_date', 'offset', 'America/New_York')\n",
    "         )\n",
    ")\n",
    "autos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key insight in Python\n",
    "# booleans are integers\n",
    "\n",
    "True + 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "False + 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining missing values\n",
    "(autos\n",
    " .isna()\n",
    " #.sum()\n",
    " .mean().mul(100).round(1)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw\n",
    " #.select_dtypes('string[pyarrow]')\n",
    " .select_dtypes(object)\n",
    " .isna()\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where are cylinders missing?\n",
    "\n",
    "(autos\n",
    " .query('cylinders.isna()')\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    " .assign(cylinders=autos.cylinders.fillna(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sklearn pipeline to fill in missing cylinders with 0\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# set output to pandas\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "# create pipeline for cylinders\n",
    "cyl_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "])\n",
    "\n",
    "# try it out\n",
    "cyl_pipe.fit_transform(autos[['cylinders']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see where it fill in missing values\n",
    "(cyl_pipe\n",
    " .fit_transform(autos[['cylinders']])\n",
    " .loc[autos.cylinders.isna()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create more realistic pipeline\n",
    "# set missing cylinders to 0 and displ to median\n",
    "from sklearn.compose import ColumnTransformer\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline.fit_transform(autos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "\n",
    "*Binning* is a process of transforming continuous numerical variables into discrete categorical 'bins'. Binning is used for:\n",
    "\n",
    "- Converting a continuous feature to a categorical feature\n",
    "- Helping with non-linear relationships\n",
    "- Reducing the effects of noise and outliers\n",
    "- Handling missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of city08\n",
    "\n",
    "autos.city08.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a probability plot to see if it is normally distributed\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "stats.probplot(autos.city08, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(autos.query('city08 < 40').city08, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the city08 data with pandas\n",
    "pd.cut(autos.city08, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.cut(autos.city08, bins=10)\n",
    " .value_counts()\n",
    " .sort_index()\n",
    " .plot.bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually create edges for bins\n",
    "(pd.cut(autos.city08, bins=[5,10,15,20,40, 140])\n",
    " .value_counts()\n",
    " .sort_index()\n",
    " .plot.bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning with sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Define the binning strategy\n",
    "binning_strategy = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('binning', binning_strategy, ['city08'])\n",
    "    ],\n",
    "    remainder='passthrough'  # This ensures other columns are left unchanged\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('transformer', column_transformer)])\n",
    "pipeline.fit_transform(autos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(autos).binning__city08.value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "binning_strategy = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('binning', binning_strategy, ['city08'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline.fit_transform(autos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace pyarrow numbers with numpy numbers\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "binning_strategy = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('binning', binning_strategy, ['city08'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline.fit_transform(autos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform\n",
    "\n",
    "Log transformation is a data transformation method in which it replaces each variable x with a log(x). This is useful when the data is skewed and you are using a model that assumes normality or linearity.\n",
    "\n",
    "It is also common to use log transformation on the target variable y in regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to show log of city08 vs original\n",
    "import numpy as np\n",
    "(autos\n",
    "    .assign(city08_log=np.log(autos.city08))\n",
    "    .plot.scatter(x='city08_log', y='city08')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos.city08.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot to show log of city08 vs original\n",
    "\n",
    "(autos\n",
    "    .assign(city08_log=np.log(autos.city08))\n",
    "    .city08_log.hist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model with linear regression to predict city08\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08']).select_dtypes('number')\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with log transform of y\n",
    "# create X and y\n",
    "X = X\n",
    "y_log = np.log(autos.city08)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline_log = Pipeline(steps=[('preprocessor', preprocessor), ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline_log.fit(X_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_log.score(X_test, y_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take exp of predictions and score the mean absolute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred_log = np.exp(pipeline_log.predict(X_test))\n",
    "\n",
    "mean_absolute_error(np.exp(y_test_log), y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "*Scaling* is an ambiguous term that generally means one of two things:\n",
    "\n",
    "-  Min-max scaling - Changing the range of a variable to be between 0 and 1 or -1 to 1.\n",
    "-  Standard scaling (Standardization) - Changing the distribution of a variable to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "We'll show examples of both below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08']).select_dtypes('number')\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('std_scaler', std_scaler),\n",
    "                           #('minmax_scaler', minmax_scaler), \n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Basic Techniques\n",
    "\n",
    "Predicting mileage from *barrels08*.\n",
    "\n",
    "- Make a model to predict *city08* from *barrels08* using linear regression.\n",
    "- What is the score?\n",
    "- Scatter plot *barrel08* vs *city08*\n",
    "- Make a new model transforming *barrels08* based on the results of the scatter plot.\n",
    "- How does the new model perform?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Basic Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Many ML algorithms cannot work with categorical data directly. The categories must be converted into numbers. This process is called *encoding*. \n",
    "\n",
    "One of the most common encodings is *one hot encoding*, also called *dummy encoding*. This creates a new column for each category with a 1 or 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autos.VClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(autos.VClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(autos.VClass, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality - number of unique values in a column\n",
    "# probably don't want to make ~5k model columns\n",
    "(autos\n",
    " .select_dtypes(object) # use 'string[pyarrow]' if using pyarrow types\n",
    " .nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinality - number of unique values in a column\n",
    "# probably don't want to make ~5k model columns\n",
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    " .index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical encoding in pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10)\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "\n",
    "        ('one_hot_encoder', one_hot_encoder, ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ],),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values AND convert Pandas 2 strings to Pandas 1 strings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False)\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('cat_imputer', cat_imputer, cat_cols),\n",
    "        ('one_hot_encoder', one_hot_encoder, cat_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train.assign(**X_train.select_dtypes('string[pyarrow]').astype(str)), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with FunctionTransformer\n",
    "# And figure out that I need a separate pipeline for categorical columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('cat_imputer', cat_imputer),\n",
    "    ('one_hot_encoder', one_hot_encoder)\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('cat_pl', cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                           ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with missing categories in test set w/ handle_unknown='ignore'\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Add handle_unknown='ignore' to OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False)\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('cat_imputer', cat_imputer),\n",
    "    ('one_hot_encoder', one_hot_encoder)\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('cat_pl', cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Encoding\n",
    "\n",
    "*Hash encoding* is a technique that can be used when there are too many categories to encode with one hot encoding. It is similar to one hot encoding, but the categories are hashed into a smaller number of columns.\n",
    "\n",
    "We are going to use the `category_encoders` library to do the encoding. This library has many other encoders that you can explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    " .pipe(lambda s: s[s > 40])\n",
    " .index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low cardinality columns\n",
    "(autos\n",
    " .select_dtypes(object)\n",
    " .nunique()\n",
    " .index\n",
    " .difference(high_cardinality_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace one hot encoder with hashing encoder for high cardinality columns\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# add hashing encoder\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        #('cat_pl', cat_pipe, cat_cols),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        ('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding\n",
    "\n",
    "From docstring: [In Target Encoding] Each category is encoded based on a shrunk estimate of the average target\n",
    "values for observations belonging to the category. The encoding scheme mixes\n",
    "the global target mean with the target mean conditioned on the value of the\n",
    "category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "te = TargetEncoder(target_type='continuous', random_state=42)\n",
    "te.fit_transform(X_train[['make']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                           ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "                          ('std_scaler', std_scaler),\n",
    "                           #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "                           ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = autos[['barrels08']]\n",
    "y = autos['city08']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[('lr', LinearRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test.assign(**X_test.select_dtypes('string[pyarrow]').astype(str)),\n",
    "                y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a custom class transformer to remove PyArrow strings (if using PyArrow)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PyArrowStringConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**X.select_dtypes('string[pyarrow]').astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),\n",
    "    ('std_scaler', std_scaler),\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Challenge\n",
    "\n",
    "Create a model to predict mileage using only the categorical columns (dropping the *model* column)\n",
    "\n",
    "```\n",
    "cat_cols = ['trany', 'drive', 'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Categorical Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction    \n",
    "\n",
    "Feature extraction is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "*Principal Component Analysis* (PCA) is a technique for reducing the dimensionality of data. It can also remove noise and might be useful as feature engineering for other algorithms. See my ML algorigthms course for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "# import pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "class PyArrowStringConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.assign(**X.select_dtypes('string[pyarrow]').astype(str))\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('std_scaler', std_scaler),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros - Noise removed, less columns\n",
    "# Cons - Less explainable\n",
    "tmp_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Aggregation\n",
    "\n",
    "Use grouping to create new features. For example, we can group by manufacturer and then calculate the average fuel economy for each manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer to add mean and std for y for a given column in X\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AddMeanStd(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col, missing_mean_val, missing_std_val):\n",
    "        self.col = col\n",
    "        # attribute names must be the same as the constructor args\n",
    "        self.missing_mean_val = missing_mean_val\n",
    "        self.missing_std_val = missing_std_val\n",
    "        # track values for each group in col\n",
    "        self.means = {}\n",
    "        self.stds = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        assert y.name not in X.columns\n",
    "        with_y = X.assign(y=y)\n",
    "        self.means = with_y.groupby(self.col)['y'].mean().to_dict()\n",
    "        self.stds = with_y.groupby(self.col)['y'].std().to_dict()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # add mean and std for each row\n",
    "        return X.assign(**{\n",
    "            f'{self.col}_mean': X[self.col].map(self.means).fillna(self.missing_mean_val),\n",
    "            f'{self.col}_std': X[self.col].map(self.stds).fillna(self.missing_std_val)\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline wtih Aggregation\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "\n",
    "*Term frequency–inverse document frequency* (TFIDF) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a feature for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].fillna('').agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('combine_str', FunctionTransformer(combine_str_cols_transformer, \n",
    "                                        kw_args={'cols': cat_cols, 'new_col_name': 'all_str'})),\n",
    "    ('tfidf', TfidfVectorizer()), # can't be sparse because of Pandas\n",
    "    ('combine_debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_combine'})),\n",
    "    ('make_dense', FunctionTransformer(lambda X: X.toarray())),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline.fit_transform(autos[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TFIDF to combination of string columns\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].fillna('').agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('combine_str', FunctionTransformer(combine_str_cols_transformer, \n",
    "                                        kw_args={'cols': cat_cols, 'new_col_name': 'all_str'})),\n",
    "    ('tfidf', TfidfVectorizer()), # can't be sparse because of Pandas\n",
    "    ('combine_debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_combine'})),\n",
    "    ('make_dense', FunctionTransformer(lambda X: X.toarray())),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "])\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "        ('text', text_pipeline, cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text pipeline to sklearn transformer so we can keep in pandas\n",
    "# and preserve index\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].fillna('').agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "\n",
    "class TextPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_cols):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.text_pipeline = Pipeline([\n",
    "            ('combine_str', FunctionTransformer(combine_str_cols_transformer, \n",
    "                                                kw_args={'cols': cat_cols, 'new_col_name': 'all_str'})),\n",
    "            ('tfidf', TfidfVectorizer()), # can't be sparse because of Pandas\n",
    "            ('make_dense', FunctionTransformer(lambda X: X.toarray())),\n",
    "            ('pca', PCA(n_components=10)),\n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.text_pipeline.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = self.text_pipeline.transform(X)\n",
    "        # replace index with X index\n",
    "        df = (res\n",
    "              .assign(index=X.index)\n",
    "              .set_index('index')\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the pipeline\n",
    "text_pipeline = TextPipeline(cat_cols)\n",
    "text_pipeline.fit(X_train, y_train)\n",
    "text_pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TFIDF to combination of string columns\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "        ('text', TextPipeline(cat_cols), cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),        \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "*Text embeddings* are a type of feature extraction that is used for text data. They are a numerical representation of text that can be used in machine learning algorithms. They are often used as a feature for text classification. A common example is a vector to represent man, woman, and king. When you add the difference between woman and man to king, you get queen.\n",
    "\n",
    "We will use the Spacy library to create text embeddings. Spacy is a library for natural language processing (NLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy language model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class SpacyEmbeddingVectorizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, columns):\n",
    "        # Load the SpaCy model\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text = (X[self.columns].fillna('').apply(lambda x: ' '.join(x), axis='columns'))\n",
    "        res = [self.nlp(row).vector for row in text]\n",
    "        df = pd.DataFrame(res, index=X.index)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out on cat_cols - takes 4+ minutes on my machine\n",
    "# using sample to speed up\n",
    "\n",
    "embeds = SpacyEmbeddingVectorizer(cat_cols)\n",
    "embeds.fit_transform(X_train.sample(1_000, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "def combine_str_cols_transformer(X, cols, new_col_name):\n",
    "    # tdidf expects a single column of strings\n",
    "    return X.assign(**{new_col_name: X[cols].agg(' '.join, axis='columns')})[new_col_name]\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "        ('text', SpacyEmbeddingVectorizer(cat_cols), cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),        \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "limit = 1_000\n",
    "pipeline.fit(X_train.iloc[:limit], y_train.iloc[:limit])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Feature Extraction\n",
    "\n",
    "Create a model that predicts mileage based on the spacy embeddings of the text columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "column_transformers = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('text', SpacyEmbeddingVectorizer(cat_cols), cat_cols)\n",
    "        ],\n",
    "        remainder='drop' # drop everything else\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "  ('preprocessor', column_transformers),\n",
    "  ('LinearRegression', LinearRegression())\n",
    "])\n",
    "\n",
    "# fit the pipeline\n",
    "limit = 1_000\n",
    "pipeline.fit(X_train.iloc[:limit], y_train.iloc[:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Features\n",
    "\n",
    "Time based data often has trends and seasonality. We can extract features from the date and time to capture these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Date and Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feature-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use feature-engine library to pull out date features\n",
    "\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "\n",
    "dtf = DatetimeFeatures(features_to_extract='all')\n",
    "dtf.fit_transform(autos[['createdOn']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality and Trend\n",
    "\n",
    "We can use the `seasonal_decompose` function from `statsmodels`` to decompose a time series into its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "seasonal_decompose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ford = (autos\n",
    "        .query(\"make == 'Ford'\")\n",
    "        #.groupby(pd.Grouper(key='createdOn', freq='ME'))\n",
    "        .groupby('year')\n",
    "        .city08\n",
    "        .median()\n",
    "        .ffill())\n",
    "ford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of 5 year decomposition (period=5 on yearly data)\n",
    "from matplotlib import pyplot as plt\n",
    "decomposition = seasonal_decompose(ford, model='additive', period=5)\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 8))\n",
    "ford.plot(ax=ax1, title='Original')\n",
    "decomposition.trend.plot(ax=ax2, title='Trend')\n",
    "decomposition.seasonal.plot(ax=ax3, title='Seasonality')\n",
    "decomposition.resid.plot(ax=ax4, title='Residuals')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def get_seasonal(group, time_col, agg_col, period=1):\n",
    "    # Sort by date and set index\n",
    "    group = group.sort_values(time_col)\n",
    "    # rename index to index\n",
    "    group.index.name = 'index'\n",
    "    ts = group.set_index(time_col)[agg_col]\n",
    "\n",
    "    # Handle groups with insufficient data\n",
    "    if len(ts) < 2:\n",
    "        return group.assign(seasonal=0, trend=0, resid=0)\n",
    "\n",
    "    # Decompose the time series\n",
    "    res = seasonal_decompose(ts, model='additive', period=period, extrapolate_trend='freq')\n",
    "\n",
    "    # Reassign the decomposed components to the group\n",
    "    return (group\n",
    "            .assign(seasonal=res.seasonal.values,\n",
    "                    trend=res.trend.values,\n",
    "                    resid=res.resid.values))\n",
    "\n",
    "def add_seasonal(df, time_col, group_cols, agg_col):\n",
    "    all_group_cols = [time_col] + group_cols\n",
    "    return (df\n",
    "            .groupby(group_cols)\n",
    "            .apply(get_seasonal, time_col=time_col, agg_col=agg_col, period=1)\n",
    "            .drop(columns=group_cols)\n",
    "            .reset_index(drop=False)\n",
    "            .set_index('index')\n",
    "            .loc[df.index]\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "res = add_seasonal(X, 'year', ['make'], agg_col='barrels08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.iloc[:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline wtih Aggregation\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# create the seasonal transformer\n",
    "class SeasonTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, time_col, group_cols, agg_col):\n",
    "        self.time_col = time_col\n",
    "        self.group_cols = group_cols\n",
    "        self.agg_col = agg_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return add_seasonal(X, self.time_col, self.group_cols, self.agg_col)\n",
    "        \n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "        ('seasonal_decompose', SeasonTransformer(time_col='year', group_cols=['make'], agg_col='barrels08')),\n",
    "#          ['year', 'make', \n",
    "#           'barrels08']),  # need to make sure we pass all columns needed\n",
    "\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model to predict monthly airline passgengers from https://www.transtats.bts.gov/Data_Elements.aspx?Data=1\n",
    "# for some reason this output is not Excel but HTML\n",
    "airlines = pd.read_html('data/Passengers_2024.xls')\n",
    "raw = airlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_airline(df_):\n",
    "  return (df_\n",
    "          .query('Month != \"TOTAL\"')\n",
    "          .astype({'Month': 'int64'})\n",
    "          .assign(date=lambda df: pd.to_datetime(df[['Year', 'Month']].assign(day=1)))\n",
    "  )\n",
    "\n",
    "air = tweak_airline(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Temporal Features\n",
    "\n",
    "Using the airline data, create a linear regression model to predict the number of passengers for the next month based on the current month.\n",
    "\n",
    "Then make another model using the `seasonal_decompose` function to add a seasonality component to the model. How do the models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "*Feature selection* is the process of selecting a subset of features that are most relevant to the target variable. There are many reasons to do this:\n",
    "\n",
    "-  Simplicity - Fewer features are easier to interpret.\n",
    "-  Memory - Fewer features require less memory to store and less computation.\n",
    "-  Speed - Fewer features result in faster algorithms.\n",
    "\n",
    "There are many techniques for feature selection. We will cover a few of them here:\n",
    "\n",
    "-  Feature importance\n",
    "-  Recursive feature elimination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance and Weights\n",
    "\n",
    "Many models can provide a feature importance or weight for each feature. This is a measure of how much the model depends on that feature. We can use this to select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline wtih Aggregation\n",
    "from sklearn.decomposition import PCA\n",
    "# replace hashing encoder with target encoder for high cardinality columns\n",
    "\n",
    "from category_encoders import hashing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, TargetEncoder\n",
    "\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create pipeline fill cylinders with 0 and displ with median\n",
    "\n",
    "cylinders_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "displ_imputer = SimpleImputer(strategy='median')\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "one_hot_encoder = OneHotEncoder(drop='first', max_categories=10, sparse_output=False, handle_unknown='ignore')\n",
    "hashing_encoder = hashing.HashingEncoder(n_components=10, drop_invariant=True)\n",
    "target_encoder = TargetEncoder(target_type='continuous', random_state=42)\n",
    "\n",
    "def debug_transformer(X, name):\n",
    "    globals()[name] = X\n",
    "    return X\n",
    "\n",
    "cat_cols =  ['make', 'model', 'trany', 'drive', \n",
    "            'VClass', 'eng_dscr', 'evMotor', 'fuelType', 'trans_dscr', ]\n",
    "low_cardinality_cols = ['VClass', 'drive', 'fuelType', 'trany']\n",
    "high_cardinality_cols = ['make', 'model', 'eng_dscr', 'evMotor', 'trans_dscr']\n",
    "\n",
    "# create the seasonal transformer\n",
    "class SeasonTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, time_col, group_cols, agg_col):\n",
    "        self.time_col = time_col\n",
    "        self.group_cols = group_cols\n",
    "        self.agg_col = agg_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return add_seasonal(X, self.time_col, self.group_cols, self.agg_col)\n",
    "        \n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cyl_imputer', cylinders_imputer, ['cylinders']),\n",
    "        ('displ_imputer', displ_imputer, ['displ']),\n",
    "        ('one_hot_encoder', one_hot_encoder, low_cardinality_cols),\n",
    "        #('hashing_encoder', hashing_encoder, high_cardinality_cols)\n",
    "        ('target_encoder', target_encoder, high_cardinality_cols),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('string_converter', PyArrowStringConverter()),\n",
    "    ('make_mean_std', AddMeanStd(col='make', missing_mean_val=0, missing_std_val=0)),\n",
    "        ('seasonal_decompose', SeasonTransformer(time_col='year', group_cols=['make'], agg_col='barrels08')),\n",
    "#          ['year', 'make', \n",
    "#           'barrels08']),  # need to make sure we pass all columns needed\n",
    "\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('debug', FunctionTransformer(debug_transformer, kw_args={'name': 'tmp_X'})),    \n",
    "    ('std_scaler', std_scaler),\n",
    "    #('pca', PCA(n_components=10)),\n",
    "\n",
    "    #  ('minmax_scaler', minmax_scaler, ['range']),\n",
    "    ('lr', LinearRegression())])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get linear regression model\n",
    "\n",
    "lr = pipeline.named_steps['lr']\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lr.coef_, index=lr.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(lr.coef_, index=lr.feature_names_in_)\n",
    " .sort_values(key=abs)\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lr.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(lr.coef_, index=lr.feature_names_in_)\n",
    " .sort_values(key=abs)\n",
    " .tail(25)\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note scale of x axis\n",
    "(pd.Series(lr.coef_, index=lr.feature_names_in_)\n",
    " .sort_values(key=abs)\n",
    " .head(26)\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost to make a model and then get feature importances\n",
    "import xgboost as xgb\n",
    "\n",
    "# create X and y\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.assign(**X.select_dtypes(object).astype('category')),\n",
    "    y, random_state=42)\n",
    "\n",
    "xg = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(xg.feature_importances_, index=X_train.columns).sort_values().tail(25).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot range vs city mpg\n",
    "(autos\n",
    " .plot.scatter(x='range', y='city08')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(xg.feature_importances_, index=X_train.columns).sort_values().iloc[:-1].plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make xg model with just range, barrels08, fuelType, and cylinders\n",
    "xg_simple = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg_simple.fit(X_train[['range', 'barrels08', 'fuelType', 'cylinders']], y_train)\n",
    "xg_simple.score(X_test[['range', 'barrels08', 'fuelType', 'cylinders']], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "*Recursive feature elimination* (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. It is a greedy optimization algorithm that aims to find the best performing feature subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# use rfe with xgboost\n",
    "xg_model = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "rfe = RFE(xg_model, n_features_to_select=3)\n",
    "\n",
    "X = autos.drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #X.assign(**X.select_dtypes('string[pyarrow]').astype('category')),\n",
    "    X.select_dtypes('number'),\n",
    "    y, random_state=42)\n",
    "\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the API to determine which features were selected\n",
    "rfe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features':X_train.columns,\n",
    "              'support': rfe.support_,\n",
    "              'ranking':rfe.ranking_}).sort_values('ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model with barrels08, range, and fuelCost08\n",
    "xg_simple = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg_simple.fit(X_train[['barrels08', 'range', 'fuelCost08']], y_train)\n",
    "xg_simple.score(X_test[['barrels08', 'range', 'fuelCost08']], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Random Column\n",
    "\n",
    "Another technique is to add a column of random numbers. We should be able to drop any columns that perform worse than the random column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and add a random column\n",
    "X = (autos\n",
    "     .drop(columns=['city08', 'highway08', 'comb08', 'createdOn', 'offset', 'str_date'])\n",
    "     .assign(random=lambda df: np.random.random(size=len(df)))\n",
    "     )\n",
    "\n",
    "y = autos.city08\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.assign(**X.select_dtypes(object).astype('category')),\n",
    "    y, random_state=42)\n",
    "\n",
    "xg = xgb.XGBRegressor(enable_categorical=True, random_state=42)\n",
    "xg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at feature importances\n",
    "# In this case the random column is the least important \n",
    "pd.Series(xg.feature_importances_, index=X_train.columns).sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Feature Selection\n",
    "\n",
    "Apply RFE to linear regression model to limit model to 5 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion - Next Steps\n",
    "\n",
    "- Practice! - Watching and listening is not enough. You need to practice what you have learned.\n",
    "- Understand your data - You need to understand your data and the problem you are trying to solve.\n",
    "- Master Pandas and Scikit-Learn - These are the most important tools for feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
